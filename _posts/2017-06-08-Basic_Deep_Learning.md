---
layout: post
title: "Basic Deep Learning"
date: 2017-06-08 13:34:26
image: '/assets/img/'
description: 'Deep Learning Basic.'
main-class: 'jekyll'
color: '#B31917'
tags:
- DeepLeaning
- Python
- Keras
categories:
twitter_text: 'Deep Learning Basic Concept.'
introduction: 'Deep Learning Basic Concept.'
---

# Basic_Deep_Learning

### ìš°ë¦¬ê°€ ì´ë¯¸ ì•Œê³  ìˆëŠ” ê²ƒ.
 - Regression : $ y = ax + b $ <- Hypothesis (ì‹)
 - Cost => Minimize
![img3](/src/0608/data/3.png)
![img1](/src/0608/data/1.PNG)


### Cost
 - $H(x) = Wx$ ê°„ë‹¨í•˜ê²Œ í‘œí˜„
 - $Cost(W) = \sum_{1}^{m}({Wx}^{i}-{y}^{i})^2$
 - W ê°’ì„ ë³€ê²½í•´ ê°€ë©´ì„œ Costê°’ì´ ê°€ì¥ ë‚®ì€ ê³³ì„ ì°¾ëŠ”ë‹¤.

## Gradient Descent (ê²½ì‚¬í•˜ê°•ë²•)
 - ìœ„ì˜ Costê°’ì„ ìµœì†Œí•œìœ¼ë¡œ í•˜ëŠ” Wë¥¼ ì°¾ì•„ë‚´ëŠ” ë°©ë²•ì„ Gradient Descentë¼ í•œë‹¤.
![img2](/src/0608/data/7.PNG)
![img2](/src/0608/data/4.png)
![img2](/src/0608/data/5.png)
![img2](/src/0608/data/6.png)

## ì‹ ê²½ë§ì˜ íŠ¹ì§•
 - ì‹ ê²½ë§ì˜ ê²½ìš° í™œì„±í™” ë˜ê³  ì•ˆë˜ê³  ì¦‰, ê²°ê³¼ê°’ì´ 0 ë˜ëŠ” 1ê°’ì„ ê°€ì§€ê²Œ ëœë‹¤.
  - ìµœì¢… Outputì˜ ê²½ìš° ì ìš© ë˜ëŠ” ë¶€ë¶„
  - Xë¼ëŠ” ê°’ìœ¼ë¡œ Yë¡œ ì¶œë ¥
  - Xë¼ëŠ” ê°’ì´ ì…ë ¥ë  ë•Œ ê°€ì¤‘ì¹˜ Wê°€ ì ìš©ë˜ê³  ê° ê°’ì— ëŒ€í•´ì„œ Bias (B)ê°€ ê°€í•´ ì§„ë‹¤.
  - ì¦‰, $W*X + b$
![img1](img/1.PNG)
![img9](data/8.PNG)
![img9](data/9.PNG)

## ìœ„ì˜ ê¸°ë³¸ ì»¨ì…‰ì„ í•¨ìˆ˜ë¡œ í‘œí˜„ê°€ëŠ¥í•˜ë‹¤ê³  ìƒê°í•˜ë‹¤ëŠ” ê²ƒì—ì„œ ì‹œì‘
![img2](img/2.PNG)

- ìœ„ì˜ í•¨ìˆ˜ ì¦‰, ë‰´ë„ì„ ê°€ì§€ê³ ëŠ” and ë˜ëŠ” or ë¬¸ì œëŠ” í•´ê²°ì´ ê°€ëŠ¥í•˜ë‹¤.
 - í•˜ì§€ë§Œ, xor ë¬¸ì œëŠ” ë‹¨ìˆœí•œ ì„ í˜• ì§ì„ ìœ¼ë¡œ í’€ ìˆ˜ ì—†ëŠ” ë¬¸ì œë¡œ ë°œê²¬. Neural Netì˜ ë°œì „ì´ ë©ˆì¶”ëŠ” ê³„ê¸°ê°€ ë˜ì—ˆë‹¤.
 - ìœ„ì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ì•„ë˜ì™€ ê°™ì€ ê·¸ë¦¼ìœ¼ë¡œ Multi Layer Perceptronìœ¼ë¡œ í•´ê²°ì´ ê°€ëŠ¥í•˜ë©°, ì´ ê°€ì¤‘ì¹˜ë¥¼ í•™ìŠµí•˜ëŠ” ë°©ë²•ì´ Back Propagationì´ë‹¤.


 - ì´ëŸ¬í•œ ê°’ì„ ì¡°ì ˆí•´ì£¼ëŠ” ê²ƒì´ $Step \ Function$ (ê³„ë‹¨ í•¨ìˆ˜ë¼ê³  í•œë‹¤.)
## ê³„ë‹¨ í•¨ìˆ˜
![img9](data/10.PNG)

### ì‹œê·¸ëª¨ì´ë“œ
![img9](data/11.PNG)

### ìŒíƒ„ì  íŠ¸
![img9](data/12.PNG)

### Relu
![img9](data/13.PNG)


#### ReLuì˜ ê²½ìš° 0~ ë¬´í•œëŒ€ ê°’ì„ ê°€ì§„ë‹¤.
 - ê·¸ëŸ¬ë¯€ë¡œ ì¤‘ê°„ Hidden layerì—ëŠ” ì‚¬ìš©í•  ìˆ˜ ìˆì§€ë§Œ ê²°ê³¼ê°’ì˜ ì¶œë ¥í•˜ëŠ” ë¶€ë¶„ì—ì„œëŠ” ì‚¬ìš© í•  ìˆ˜ ì—†ë‹¤.


![img3](/src/0608/img/3.PNG)

## XOR ë¬¸ì œ
![img4](/src/0608/img/4.PNG)
![img5](/src/0608/img/5.PNG)
![img6](/src/0608/img/6.PNG)
![img7](/src/0608/img/7.PNG)

## BackPropagation
![img13](/src/0608/img/13.PNG)

### Cost Minimize
 - Gradient Descent ë¥¼ ì´ìš©í•˜ì—¬ Cost ë¥¼ ê°ì†Œ ì‹œì¼œì•¼ í•œë‹¤.

![img14](/src/0608/img/14.PNG)

## ë¯¸ë¶„ íŠ¹ì§•
 - <a href="https://youtu.be/oZyvmtqLmLo">Sung Kimêµìˆ˜ì˜ ë¯¸ë¶„ ê°•ì˜ </a>

![img15](/src/0608/img/15.PNG)

![img16](/src/0608/img/16.PNG)

#### ìœ„ì˜ ë¯¸ë¶„ ê²°ê³¼ê°’ì— ì˜í•´ ê°€ì¤‘ì¹˜ë“¤ì„ ì¡°ì ˆí•  ìˆ˜ê°€ ìˆë‹¤.
 - ë‚´ê°€ ì›í•˜ëŠ” ê°’ë³´ë‹¤ í¬ê²Œ ë‚˜ì™”ë‹¤ë©´ - ì˜í–¥ì´ ìˆëŠ” ë†ˆì„ ë†’ì´ê³  + ì˜í–¥ì´ ìˆëŠ” ë†ˆì„ ë‚®ì¶”ë©´ ëœë‹¤.

### But Back Propagationì—ì„œ Sigmoidë¥¼ ì‚¬ìš©í–ˆì„ ê²½ìš° ë‹¨ì ì´ ë°œìƒ
 - backpropagation ì´ ì—„ì²­ë‚œ ë ˆì´ì–´ë¡œ ìˆ˜ì„±ë˜ë©´ ì‘ë™ì´ ì˜ ì•ˆëë‹¤ ê·¸ ì´ìœ ê°€ ê°’ë“¤ì´ 0.ëª‡ ì´ëŸ¬í•œ í˜•íƒœë¡œë§Œ ì¶œë ¥ë˜ì„œ ë’¤ë¡œ ê°€ë©´ ê°ˆìˆ˜ë¡ ì•ì˜ ê°’ì´ ì „ë‹¬ì´ ì˜ ì•ˆë˜ì„œ í•™ìŠµì´ ì–´ë ¤ì› ë‹¤.
 - ì´ëŸ¬í•œ ë¬¸ì œì ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ë‚˜ì˜¨ ê²ƒì´ ReLu ë¼ëŠ” Step Function (ì´ë¯¸ì§€ëŠ” ì§‘ì— ìˆëŠ” ê²ƒ ì“°ê¸°.)

### ReLu
 - 0 ì´ìƒì˜ ê°’ì´ ì…ë ¥ ëì„ ê²½ìš° x -> xë¡œ ê°’ì´ ì¶œë ¥.
 - 0 ~ 1ê°’ë§Œ ì·¨ê¸‰í•˜ë˜ Sigmoidì— ë¹„í•´ ê°’ì´ í¬ê²Œ ì ìš© ë˜ë¯€ë¡œ ê°’ì˜ ì „ë‹¬ì´ ê¹Šê³  ë„“ê²Œ ì ìš© ë  ìˆ˜ ìˆë‹¤.

# í•™êµ DeepLearning ë³µìŠµ
## MNIST
 - ì†ê¸€ì”¨ë¡œ ëœ ê·¸ë¦¼ ë°ì´í„° ë§ì¶”ê¸°
 - 0 ~ 9 ê¹Œì§€ ìˆ«ì
 - train, test Set ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì ¸ ìˆìŒ
 - Train : 60000ê°œ, Test : 10000ê°œ


```python
import keras
```

    Using TensorFlow backend.


### MNIST ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°


```python
from keras.datasets import mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
```

    Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz



```python
x_train.shape
```




    (60000, 28, 28)



#### Train
  - (60000, 28, 28)
  - 60000 : ë°ì´í„° ì…‹ ê°¯ìˆ˜
  - 28 : width(ê°€ë¡œ ê¸¸ì´)
  - 28 : height(ì„¸ë¡œ ê¸¸ì´)


```python
x_test.shape
```




    (10000, 28, 28)



#### Test
 - 10000ê°œ ë°ì´í„°
 - ìœ„ì™€ ë™ì¼í•˜ê²Œ 28 x 28 ë°ì´í„°


```python
n_train, width, height = x_train.shape
```

 - Python ë¬¸ë²•ì—ì„œ _ <- ëŠ” í•„ìš” ì—†ëŠ” ë¶€ë¶„ì„ ë²„ë¦¬ëŠ” ì—­í• ì„ í•œë‹¤.
  - ì™œ? ì´ëŸ¬í•œ ë¬¸ë²•ì„ í˜„ì¬ ì‚¬ìš©í•˜ëŠ”ê°€.
  - ì´ë¯¸ ìœ„ì—ì„œ testë¶€ë¶„ì—ì„œ width, heightë¥¼ ê°€ì ¸ ì™”ê¸° ë•Œë¬¸ì— íŒ¨ìŠ¤í•˜ëŠ” ë¶€ë¶„


```python
n_test, _, _ = x_test.shape
```

### ë°ì´í„° ë³´ê¸°


```python
%matplotlib inline
```


```python
import matplotlib.pyplot as plt
```


```python
plt.imshow(x_train[4,], cmap='gray')
```




    <matplotlib.image.AxesImage at 0x20ff31adc50>




![png](output_24_1.png)



```python
y_train[4,]
```




    9



### ë°ì´í„° ì „ì²˜ë¦¬
#### ì…ë ¥ (Input Layer)
 - ì…ë ¥ ë¶€ë¶„ì—ì„œ í˜„ì¬ ë‚˜ì™€ìˆëŠ” í˜•íƒœ
 - ë°ì´í„° ëª¨ì–‘ì„ 28 * 28 í˜•íƒœë¡œ ë„£ì„ ìˆ˜ê°€ ì—†ìœ¼ë¯€ë¡œ
 - 28 / 28 / 28 / 28 / 28 ~~~~~~~~~~~~~~~ 28ê°œ í˜•íƒœë¡œ ë¶™ì—¬ì„œ í•œì¤„ë¡œ ë§Œë“¤ì–´ì„œ ì…ë ¥ ìœ¼ë¡œ ë§Œë“ ë‹¤.
 - ì˜ˆì œ


```python
import numpy as np
```


```python
a = np.array([0,1,0,1,1,1,0])
```


```python
b = np.array([[0],
    [1],
    [1],
    [0],
    [1],
    [0],
    [1]])
```


```python
a.shape
```




    (7,)




```python
b.shape
```




    (7, 1)




```python
np.dot(a,b)## ì´ëŸ¬í•œ í˜•íƒœë¡œ matrix ê³±ì„ í•´ì£¼ê¸° ìœ„í•´ì„œ
```




    array([2])




```python
x_train[4,] # ì´ëŸ¬í•œ í˜•íƒœë¡œëŠ” Neural Netì„ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤. ê·¸ë ‡ê¸°ì—
```




    array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55,
            148, 210, 253, 253, 113,  87, 148,  55,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  87, 232,
            252, 253, 189, 210, 252, 252, 253, 168,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,  57, 242, 252,
            190,  65,   5,  12, 182, 252, 253, 116,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,  96, 252, 252, 183,
             14,   0,   0,  92, 252, 252, 225,  21,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0, 132, 253, 252, 146,  14,
              0,   0,   0, 215, 252, 252,  79,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0, 126, 253, 247, 176,   9,   0,
              0,   8,  78, 245, 253, 129,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,  16, 232, 252, 176,   0,   0,   0,
             36, 201, 252, 252, 169,  11,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,  22, 252, 252,  30,  22, 119, 197,
            241, 253, 252, 251,  77,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,  16, 231, 252, 253, 252, 252, 252,
            226, 227, 252, 231,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,  55, 235, 253, 217, 138,  42,
             24, 192, 252, 143,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
             62, 255, 253, 109,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
             71, 253, 252,  21,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0, 253, 252,  21,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
             71, 253, 252,  21,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
            106, 253, 252,  21,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
             45, 255, 253,  21,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0, 218, 252,  56,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,  96, 252, 189,  42,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,  14, 184, 252, 170,  11,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,  14, 147, 252,  42,   0,   0,   0,   0,   0,   0,   0,
              0,   0],
           [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
              0,   0]], dtype=uint8)



 - 60000ê°œì˜ ë°ì´í„°ë¥¼ 28 * 28 = 784 ê°œ ë°ì´í„°ë¡œ ì­‰ ê¸¸ì— í’€ì–´ì„œ ë„£ì–´ì•¼ í•œë‹¤.


```python
input_train = x_train.reshape(n_train, width*height)
```


```python
input_train.astype('float32')
```




    array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],
           [ 0.,  0.,  0., ...,  0.,  0.,  0.],
           [ 0.,  0.,  0., ...,  0.,  0.,  0.],
           ...,
           [ 0.,  0.,  0., ...,  0.,  0.,  0.],
           [ 0.,  0.,  0., ...,  0.,  0.,  0.],
           [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)




```python
input_train = input_train / 255.0  # 0~255ì˜ í¬ê¸°ë¡œ ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ”ë° í•´ë‹¹ ë°ì´í„°ë¥¼ í¬ê²Œ í•  ê²½ìš° í•™ìŠµì— ì–´ë ¤ì›€ì´ ìˆë‹¤. Normalization
```


```python
input_train.max()
```




    1.0



- í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ë„ ë™ì¼í•˜ê²Œ ì²˜ë¦¬


```python
input_test = x_test.reshape(n_test, width*height)
input_test.astype('float32')
input_test = input_test / 255.0
```

#### ì¶œë ¥
 - 0ì´ë¼ëŠ” ë¬¸ìê°€ ìˆë‹¤. ì™¼ìª½ ë°˜ì„ ì§€ìš°ë©´ 1ì´ëœë‹¤ í•˜ì§€ë§Œ ë˜ ë°˜ì„ ì§€ìš´ë‹¤ê³ í•´ì„œ 2ê°€ ë˜ì§€ëŠ” ì•ŠëŠ”ë‹¤.
 - ìˆ«ìëŠ” ë²”ì£¼í˜• ë°ì´í„°ì´ë‹¤. (ê·¸ë¦¼ì€)


```python
output_train = keras.utils.to_categorical(y_train,10) # 0~9ê¹Œì§€ë¥¼ ì¹´í…Œê³ ë¦¬ë¡œ ë³€í™˜
```


```python
output_train # one-hot encoding
```




    array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],
           [ 1.,  0.,  0., ...,  0.,  0.,  0.],
           [ 0.,  0.,  0., ...,  0.,  0.,  0.],
           ...,
           [ 0.,  0.,  0., ...,  0.,  0.,  0.],
           [ 0.,  0.,  0., ...,  0.,  0.,  0.],
           [ 0.,  0.,  0., ...,  0.,  1.,  0.]])




```python
output_test = keras.utils.to_categorical(y_test, 10)
```

#### ê°„ë‹¨í•œ ëª¨ë¸


```python
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import RMSprop
```

## softmax
 - What's the SoftMax Function
 - ìš°ë¦¬ëŠ” Logistic Regressionì„ ì•Œê³  ìˆë‹¤. (A or B / 1 or 0) ê³¼ ê°™ì´ 2ê°œì˜ ê°’ì„ êµ¬ë³„í•  ë•Œ ì‚¬ìš©í•˜ëŠ” Step function
 - ì´ëŸ¬í•œ ë‚´ìš©ì„ Muliti Categorial Label ì„ êµ¬ë³„í•  ë•ŒëŠ” ë‹¨ë…ìœ¼ë¡œëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤.

![img17](/src/0608/img/17.PNG)
![img18](/src/0608/img/18.PNG)
![img19](/src/0608/img/19.PNG)
![img20](/src/0608/img/20.PNG)
![img21](/src/0608/img/21.PNG)
![img22](/src/0608/img/22.PNG)
![img23](/src/0608/img/23.PNG)
![img24](/src/0608/img/24.PNG)
![img25](/src/0608/img/25.PNG)
![img26](/src/0608/img/26.PNG)
![img27](/src/0608/img/27.PNG)
![img28](/src/0608/img/28.PNG)


```python
model = Sequential()
model.add(Dense(392, activation='tanh', input_shape=(784,)))
model.add(Dense(10, activation='softmax'))
```

### Activation : tanh graph
![img9](/src/0608/data/12.PNG)


```python
model.summary() # dense_1 ì—ëŠ” 784 * 392 + 392 biasê°€ ìˆë‹¤. ë¼ëŠ” param
```

    _________________________________________________________________
    Layer (type)                 Output Shape              Param #
    =================================================================
    dense_1 (Dense)              (None, 392)               307720
    _________________________________________________________________
    dense_2 (Dense)              (None, 10)                3930
    =================================================================
    Total params: 311,650
    Trainable params: 311,650
    Non-trainable params: 0
    _________________________________________________________________


 - loss : ìš°ë¦¬ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ë¶€ì •í™•í•˜ëƒ
  - mean_squared_error : MSE  âˆ’1/ğ‘ âˆ‘(ğ‘¦âˆ’ğ‘¦Â Ì‚ )^2
  - cross entropy : ë‚´ê°€ ë­”ê°€ë¥¼ ë§ì¶œë•Œ ë†’ì€ í™•ë¥ ë¡œ ë§ì¶˜ê±°ë¥¼ ì¢‹ì•„í•´, ë‚®ì€ í™•ë¥ ë¡œ ë§ì¶˜ ê²ƒì€ lossê°€ ì»¤ì§„ë‹¤. ë°°íŒ…ì„ í¬ê²Œ í–ˆëŠ”ë° ëª»ë§ì¶”ë©´ loss ë˜í•œ ì»¤ì§„ë‹¤.


```python
model.compile(loss='categorical_crossentropy',
              optimizer=RMSprop(),
              metrics=['accuracy'])
```

### íŠ¸ë ˆì´ë‹


```python
batch_size = 128 # í•œë²ˆì— ë“¤ì–´ê°€ëŠ” ë°ì´í„° í¬ê¸°
epochs = 1 # ë°˜ë³µíšŸìˆ˜
```


```python
history = model.fit(input_train, output_train,
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1,
                    validation_data=(input_test, output_test))
```

    Train on 60000 samples, validate on 10000 samples
    Epoch 1/1
    60000/60000 [==============================] - 4s - loss: 0.3328 - acc: 0.9016 - val_loss: 0.2330 - val_acc: 0.9312



```python
history.history
```




    {'acc': [0.90156666673024499],
     'loss': [0.33278176914850871],
     'val_acc': [0.93120000000000003],
     'val_loss': [0.23297160456180571]}



### í‰ê°€


```python
score = model.evaluate(input_test, output_test, verbose=0)
score
```




    [0.23297160540521145, 0.93120000000000003]



# Comming up Next!!!

## CNN
 - ê³ ì–‘ì´ì˜ ì‹œì‹ ê²½ì„ ê´€ì°°
 - ê·¸ë¦¼ì„ ë³¼ë•Œ ì „ì²´ ëˆ„ëŸ°ì´ í™œì„±í™” ë˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì¼ë¶€ë§Œ í™œì„±í™”ê°€ë˜ê³  ë‹¤ë¥¸ ê·¸ë¦¼ì„ ë³´ì•¼ì£¼ë©´ ë˜ ë‹¤ë¥¸ ë‰´ëŸ°ì´ í™œì„±í™”ê°€ ë˜ëŠ” í˜•íƒœë¥¼ ë³´ì„
 - ì¦‰, ì´ ì‹œì‹ ê²½ë“¤ì´ ì „ì²´ë¥¼ ë³´ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì¼ë¶€ë¥¼ ë³´ê³  ë‚˜ì¤‘ì— ì¡°í•©í•˜ëŠ” ê²ƒì´ ì•„ë‹Œê°€ í•˜ëŠ”ê²ƒì´ cnn ì˜ ê°œë…
